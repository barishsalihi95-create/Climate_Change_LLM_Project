{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14629776,"sourceType":"datasetVersion","datasetId":9345325}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-26T20:01:30.460606Z","iopub.execute_input":"2026-01-26T20:01:30.460845Z","iopub.status.idle":"2026-01-26T20:01:30.777703Z","shell.execute_reply.started":"2026-01-26T20:01:30.460810Z","shell.execute_reply":"2026-01-26T20:01:30.776751Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/climate-change-qa-enlarged/climate_change_qa_enlarged.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n# ONE-CELL Kaggle: Local deps override + GPT-2 Fine-tune (Train/Val + Val Loss) + Save + Demo\n# Fixes: huggingface-hub version mismatch with system transformers (>=0.34,<1.0)\n# Also: auto-fallback to CPU if P100 kernel mismatch occurs (\"no kernel image\")\n# ============================================================\n\nimport os, sys, subprocess, shutil\n\nDEPS_DIR = \"/kaggle/working/_deps\"\n\ndef fresh_deps_dir():\n    if os.path.isdir(DEPS_DIR):\n        shutil.rmtree(DEPS_DIR, ignore_errors=True)\n    os.makedirs(DEPS_DIR, exist_ok=True)\n    if DEPS_DIR not in sys.path:\n        sys.path.insert(0, DEPS_DIR)\n\ndef pip_install_local(pkgs):\n    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-cache-dir\", \"-t\", DEPS_DIR] + pkgs\n    subprocess.check_call(cmd)\n\ndef force_local_imports():\n    # Remove already-loaded modules so they re-import from DEPS_DIR\n    for mod in list(sys.modules.keys()):\n        if mod.startswith((\"huggingface_hub\", \"fsspec\")):\n            del sys.modules[mod]\n\ndef setup_local_overrides():\n    fresh_deps_dir()\n    # Key fix: transformers 4.57.1 wants huggingface-hub >=0.34,<1.0\n    pip_install_local([\n        \"huggingface-hub==0.35.1\",\n        \"fsspec[http]==2025.10.0\",\n    ])\n    force_local_imports()\n\n    import huggingface_hub, fsspec\n    print(\"‚úÖ huggingface_hub:\", huggingface_hub.__version__, \"from\", huggingface_hub.__file__)\n    print(\"‚úÖ fsspec:\", fsspec.__version__, \"from\", fsspec.__file__)\n\nsetup_local_overrides()\n\n# ----------------------------\n# Imports (NO Trainer!)\n# ----------------------------\nimport json\nimport math\nimport random\nfrom typing import Dict, List, Tuple\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Subset\n\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom transformers.optimization import get_linear_schedule_with_warmup\n\n# ----------------------------\n# Config\n# ----------------------------\nDATA_PATH = \"/kaggle/input/climate-change-qa-enlarged/climate_change_qa_enlarged.json\"\nMODEL_NAME = \"gpt2\"\nSAVE_DIR = \"./fine_tuned_gpt2\"\n\nMAX_LENGTH = 256\nEPOCHS = 3\nBATCH_SIZE = 4\nEVAL_BATCH_SIZE = 8\nGRAD_ACCUM = 2\nLR = 5e-5\nWARMUP_RATIO = 0.06\nWEIGHT_DECAY = 0.01\nSEED = 42\nLOG_EVERY = 50\nVAL_RATIO = 0.10\n\n# ----------------------------\n# Utilities\n# ----------------------------\ndef set_seed(seed: int) -> None:\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef load_json_qa(path: str) -> List[Dict[str, str]]:\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    if not isinstance(data, list) or not data:\n        raise ValueError(\"Dataset JSON must be a non-empty list of {question, answer}.\")\n    if \"question\" not in data[0] or \"answer\" not in data[0]:\n        raise ValueError(\"Each item must contain keys: 'question' and 'answer'.\")\n    return data\n\ndef train_val_split(n: int, val_ratio: float, seed: int) -> Tuple[List[int], List[int]]:\n    idx = list(range(n))\n    rng = random.Random(seed)\n    rng.shuffle(idx)\n    val_n = max(1, int(n * val_ratio))\n    return idx[val_n:], idx[:val_n]\n\n@torch.no_grad()\ndef eval_loss(model: GPT2LMHeadModel, dataloader: DataLoader, device: torch.device) -> float:\n    model.eval()\n    total = 0.0\n    steps = 0\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(**batch)\n        total += out.loss.item()\n        steps += 1\n    return total / max(1, steps)\n\n@torch.no_grad()\ndef generate_answer(model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, question: str, max_new_tokens: int = 80) -> str:\n    device = next(model.parameters()).device\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    out = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=True,\n        top_p=0.9,\n        temperature=0.8,\n        repetition_penalty=1.15,\n        pad_token_id=tokenizer.eos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n    text = tokenizer.decode(out[0], skip_special_tokens=True)\n    return text.split(\"### Answer:\", 1)[1].strip() if \"### Answer:\" in text else text.strip()\n\n# ----------------------------\n# Dataset\n# ----------------------------\nclass QACausalLMDataset(Dataset):\n    def __init__(self, items: List[Dict[str, str]], tokenizer: GPT2Tokenizer, max_length: int):\n        self.items = items\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self) -> int:\n        return len(self.items)\n\n    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n        q = str(self.items[idx][\"question\"]).strip()\n        a = str(self.items[idx][\"answer\"]).strip()\n        text = f\"### Question:\\n{q}\\n\\n### Answer:\\n{a}\"\n\n        enc = self.tokenizer(\n            text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n        input_ids = enc[\"input_ids\"].squeeze(0)\n        attention_mask = enc[\"attention_mask\"].squeeze(0)\n        labels = input_ids.clone()\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\n# ----------------------------\n# Robust device selection\n# ----------------------------\ndef choose_device() -> torch.device:\n    if not torch.cuda.is_available():\n        return torch.device(\"cpu\")\n    try:\n        _ = (torch.tensor([1.0], device=\"cuda\") * 2.0)\n        torch.cuda.synchronize()\n        return torch.device(\"cuda\")\n    except Exception as e:\n        print(\"‚ö†Ô∏è CUDA present but not runnable:\", repr(e))\n        print(\"‚û°Ô∏è Falling back to CPU.\")\n        return torch.device(\"cpu\")\n\n# ----------------------------\n# Train loop\n# ----------------------------\ndef main() -> None:\n    set_seed(SEED)\n    device = choose_device()\n    print(\"Device:\", device)\n\n    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n    model.resize_token_embeddings(len(tokenizer))\n    model.to(device)\n\n    items = load_json_qa(DATA_PATH)\n    full_ds = QACausalLMDataset(items, tokenizer, MAX_LENGTH)\n\n    train_idx, val_idx = train_val_split(len(full_ds), VAL_RATIO, SEED)\n    train_ds = Subset(full_ds, train_idx)\n    val_ds = Subset(full_ds, val_idx)\n\n    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n    val_dl = DataLoader(val_ds, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=2)\n\n    total_steps = math.ceil(len(train_dl) / GRAD_ACCUM) * EPOCHS\n    warmup_steps = int(total_steps * WARMUP_RATIO)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n\n    use_amp = (device.type == \"cuda\")\n    scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n\n    print(f\"Train size: {len(train_ds)} | Val size: {len(val_ds)}\")\n    print(f\"Steps: {total_steps} | Warmup: {warmup_steps} | AMP: {use_amp}\")\n\n    global_step = 0\n    running = 0.0\n\n    for epoch in range(1, EPOCHS + 1):\n        print(f\"\\n=== Epoch {epoch}/{EPOCHS} ===\")\n        model.train()\n        optimizer.zero_grad(set_to_none=True)\n\n        for step, batch in enumerate(train_dl, start=1):\n            batch = {k: v.to(device) for k, v in batch.items()}\n\n            try:\n                with torch.amp.autocast(\"cuda\", enabled=use_amp):\n                    out = model(**batch)\n                    loss = out.loss / GRAD_ACCUM\n\n                scaler.scale(loss).backward()\n                running += loss.item()\n\n                if step % GRAD_ACCUM == 0:\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad(set_to_none=True)\n                    scheduler.step()\n\n                    global_step += 1\n                    if global_step % LOG_EVERY == 0:\n                        print(f\"step={global_step} train_loss={running/LOG_EVERY:.4f}\")\n                        running = 0.0\n\n            except RuntimeError as e:\n                if \"no kernel image\" in str(e).lower():\n                    print(\"\\n‚ö†Ô∏è GPU kernel mismatch detected. Switching to CPU and continuing...\")\n                    device = torch.device(\"cpu\")\n                    model.to(device)\n                    use_amp = False\n                    scaler = torch.amp.GradScaler(\"cuda\", enabled=False)\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    out = model(**batch)\n                    (out.loss / GRAD_ACCUM).backward()\n                else:\n                    raise\n\n        vloss = eval_loss(model, val_dl, device)\n        print(f\"Epoch {epoch} validation_loss={vloss:.4f}\")\n        print(\"\\n[Demo] Q: What is climate change?\")\n        print(\"[Demo] A:\", generate_answer(model, tokenizer, \"What is climate change?\"))\n\n    print(\"\\nSaving to:\", SAVE_DIR)\n    model.save_pretrained(SAVE_DIR)\n    tokenizer.save_pretrained(SAVE_DIR)\n    print(\"‚úÖ Saved.\")\n\n    print(\"\\n--- FINAL INFERENCE DEMO ---\")\n    model.eval()\n    for q in [\n        \"What are greenhouse gases?\",\n        \"How does deforestation contribute to climate change?\",\n        \"How can individuals reduce their carbon footprint?\",\n    ]:\n        print(\"\\nQ:\", q)\n        print(\"A:\", generate_answer(model, tokenizer, q))\n\nmain()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-26T20:06:36.673101Z","iopub.execute_input":"2026-01-26T20:06:36.673710Z","iopub.status.idle":"2026-01-26T20:07:30.179152Z","shell.execute_reply.started":"2026-01-26T20:06:36.673680Z","shell.execute_reply":"2026-01-26T20:07:30.178404Z"}},"outputs":[{"name":"stdout","text":"     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 57.7/57.7 kB 3.9 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75.1/75.1 kB 31.0 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 563.3/563.3 kB 22.3 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 201.0/201.0 kB 348.5 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.8/1.8 MB 98.7 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.3/3.3 MB 143.4 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 74.4/74.4 kB 320.8 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 807.9/807.9 kB 406.3 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 78.5/78.5 kB 298.0 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44.6/44.6 kB 300.1 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 64.7/64.7 kB 332.6 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 67.6/67.6 kB 234.5 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 152.9/152.9 kB 289.7 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 153.5/153.5 kB 379.7 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 242.4/242.4 kB 351.4 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 71.0/71.0 kB 328.2 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 256.3/256.3 kB 374.4 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 221.6/221.6 kB 393.9 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 131.6/131.6 kB 367.0 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 377.3/377.3 kB 395.7 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nlangchain-core 0.3.79 requires packaging<26.0.0,>=23.2.0, but you have packaging 26.0 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ huggingface_hub: 0.35.1 from /kaggle/working/_deps/huggingface_hub/__init__.py\n‚úÖ fsspec: 2025.10.0 from /kaggle/working/_deps/fsspec/__init__.py\n","output_type":"stream"},{"name":"stderr","text":"2026-01-26 20:06:54.460773: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769458014.658810     124 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769458014.714019     124 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769458015.201064     124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769458015.201095     124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769458015.201098     124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769458015.201100     124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fedbb5187bd4461be06252b88a1005d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d117d260744e4cd9adec1082af9d2c63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48b988ce807648bc84da8e10fb3a0e6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5f8337054f24d36810320602996171a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dc976cf8777403d86fdf56046810770"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f04a5ea9a60d435385543075146cbf9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c3a20c8c46e46c7a28207f6b367a9e5"}},"metadata":{}},{"name":"stdout","text":"Train size: 117 | Val size: 13\nSteps: 45 | Warmup: 2 | AMP: True\n\n=== Epoch 1/3 ===\n","output_type":"stream"},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 validation_loss=0.4477\n\n[Demo] Q: What is climate change?\n[Demo] A: This issue arose in the late 1970s when a global warming alarmism and political pressure were building to control carbon dioxide emissions.\n\n=== Epoch 2/3 ===\nEpoch 2 validation_loss=0.2892\n\n[Demo] Q: What is climate change?\n[Demo] A: Climate change affects the weather, which causes extreme precipitation events that cause flooding and storms.\n\n=== Epoch 3/3 ===\nEpoch 3 validation_loss=0.2777\n\n[Demo] Q: What is climate change?\n[Demo] A: Climate Change affects ecosystems, affecting wildlife and the environment. It disrupts food supply chains that sustain human activities, such as logging and agriculture; destroys livelihood-building opportunities for people; or causes widespread famine across many countries.\n\nSaving to: ./fine_tuned_gpt2\n‚úÖ Saved.\n\n--- FINAL INFERENCE DEMO ---\n\nQ: What are greenhouse gases?\nA: Greenhouse gas emissions from burning fossil fuels, such as coal, generate heat and produce carbon dioxide.\n\nQ: How does deforestation contribute to climate change?\nA: Droughts affect ecosystems and habitats, including forests, lakes or streams.\n\nQ: How can individuals reduce their carbon footprint?\nA: Individuals who eliminate emissions are reducing the amount of energy they use by cutting consumption, improving efficiency and lowering household costs.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================\n# Kaggle Notebook UI: Textbox -> Search question in dataset -> Show answer\n# Works with your JSON format: [{\"question\": \"...\", \"answer\": \"...\"}, ...]\n# ============================================================\n\nimport json\nimport re\nfrom difflib import SequenceMatcher\n\nimport ipywidgets as widgets\nfrom IPython.display import display, Markdown, clear_output\n\nDATA_PATH = \"/kaggle/input/climate-change-qa-enlarged/climate_change_qa_enlarged.json\"\n\ndef load_qa(path: str):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    if not isinstance(data, list):\n        raise ValueError(\"Dataset JSON must be a list of objects.\")\n    for i, item in enumerate(data[:5]):\n        if \"question\" not in item or \"answer\" not in item:\n            raise ValueError(f\"Item {i} missing 'question' or 'answer'.\")\n    return data\n\nQA = load_qa(DATA_PATH)\n\ndef normalize(text: str) -> str:\n    text = text.strip().lower()\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    return text\n\n# Precompute normalized questions for faster search\nNQ = [normalize(x[\"question\"]) for x in QA]\n\ndef best_match(query: str, top_k: int = 5):\n    qn = normalize(query)\n    if not qn:\n        return []\n\n    # 1) Exact normalized match first\n    exact = [i for i, nq in enumerate(NQ) if nq == qn]\n    if exact:\n        return [(1.0, QA[exact[0]])]\n\n    # 2) Contains match (fast heuristic)\n    contains = [(0.92, QA[i]) for i, nq in enumerate(NQ) if qn in nq or nq in qn]\n    if contains:\n        return contains[:top_k]\n\n    # 3) Fuzzy match\n    scored = []\n    for i, nq in enumerate(NQ):\n        s = SequenceMatcher(None, qn, nq).ratio()\n        if s >= 0.55:  # threshold\n            scored.append((s, QA[i]))\n    scored.sort(key=lambda x: x[0], reverse=True)\n    return scored[:top_k]\n\n# ----------------------------\n# UI widgets\n# ----------------------------\ntitle = widgets.HTML(\"<h3>Climate Change Q&A Search</h3>\")\n\nquestion_box = widgets.Text(\n    value=\"\",\n    placeholder=\"Type your question here...\",\n    description=\"Question:\",\n    layout=widgets.Layout(width=\"90%\"),\n)\n\nsearch_btn = widgets.Button(\n    description=\"Find Answer\",\n    button_style=\"success\",\n    tooltip=\"Search the dataset for the best matching question\",\n)\n\ntopk_slider = widgets.IntSlider(\n    value=3,\n    min=1,\n    max=10,\n    step=1,\n    description=\"Top-K:\",\n    continuous_update=False,\n)\n\noutput = widgets.Output()\n\ndef on_search(_):\n    with output:\n        clear_output()\n        q = question_box.value.strip()\n        if not q:\n            display(Markdown(\"‚ö†Ô∏è Please type a question.\"))\n            return\n\n        matches = best_match(q, top_k=topk_slider.value)\n        if not matches:\n            display(Markdown(\"‚ùå No good match found in the dataset. Try rephrasing.\"))\n            return\n\n        best_score, best_item = matches[0]\n        display(Markdown(f\"### ‚úÖ Best Match (score: `{best_score:.3f}`)\"))\n        display(Markdown(f\"**Dataset Question:** {best_item['question']}\"))\n        display(Markdown(f\"**Answer:** {best_item['answer']}\"))\n\n        if len(matches) > 1:\n            display(Markdown(\"---\\n### Other close matches\"))\n            for s, item in matches[1:]:\n                display(Markdown(f\"- score `{s:.3f}` ‚Äî **Q:** {item['question']}\"))\n\nsearch_btn.on_click(on_search)\n\nui = widgets.VBox([\n    title,\n    widgets.HBox([question_box, search_btn]),\n    topk_slider,\n    output\n])\n\ndisplay(ui)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-26T20:20:49.180875Z","iopub.execute_input":"2026-01-26T20:20:49.182019Z","iopub.status.idle":"2026-01-26T20:20:49.220288Z","shell.execute_reply.started":"2026-01-26T20:20:49.181977Z","shell.execute_reply":"2026-01-26T20:20:49.219529Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<h3>Climate Change Q&A Search</h3>'), HBox(children=(Text(value='', description='Qu‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e51a5ff625441baaac8823e4f58f6f0"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# ============================================================\n# Kaggle UI: Textbox Q -> Dataset search (exact+fuzzy+semantic) -> Answer\n# NEW:\n#  - Confidence label (High/Medium/Low) based on score + method\n#  - Toggle: show Dataset answer, GPT answer, or BOTH side-by-side\n# Dataset: /kaggle/input/climate-change-qa-enlarged/climate_change_qa_enlarged.json\n# Optional fine-tuned GPT-2: ./fine_tuned_gpt2\n# ============================================================\n\nimport os\nimport json\nimport re\nimport time\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Tuple, Optional\nfrom difflib import SequenceMatcher\n\nimport torch\nimport torch.nn.functional as F\n\nimport ipywidgets as widgets\nfrom IPython.display import display, Markdown, clear_output\n\nfrom transformers import AutoTokenizer, AutoModel, GPT2LMHeadModel, GPT2Tokenizer\n\n# ----------------------------\n# Config\n# ----------------------------\nDATA_PATH = \"/kaggle/input/climate-change-qa-enlarged/climate_change_qa_enlarged.json\"\nFINETUNED_DIR = \"./fine_tuned_gpt2\"\n\nEMBED_MODEL_NAME = \"distilbert-base-uncased\"\nEMBED_MAX_LEN = 192\nEMBED_BATCH = 32\n\nFUZZY_THRESHOLD = 0.62\nSEMANTIC_THRESHOLD_DEFAULT = 0.45\nTOPK_DEFAULT = 3\n\n# ----------------------------\n# Device helpers (robust)\n# ----------------------------\ndef choose_device() -> torch.device:\n    if not torch.cuda.is_available():\n        return torch.device(\"cpu\")\n    try:\n        _ = (torch.tensor([1.0], device=\"cuda\") * 2.0)\n        torch.cuda.synchronize()\n        return torch.device(\"cuda\")\n    except Exception:\n        return torch.device(\"cpu\")\n\nDEVICE = choose_device()\n\n# ----------------------------\n# Dataset\n# ----------------------------\ndef load_qa(path: str) -> List[Dict[str, str]]:\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    if not isinstance(data, list) or not data:\n        raise ValueError(\"Dataset JSON must be a non-empty list of objects with keys {question, answer}.\")\n    for i, item in enumerate(data[:5]):\n        if \"question\" not in item or \"answer\" not in item:\n            raise ValueError(f\"Dataset item {i} missing 'question' or 'answer'.\")\n    return data\n\nQA: List[Dict[str, str]] = load_qa(DATA_PATH)\n\ndef normalize(text: str) -> str:\n    text = text.strip().lower()\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    return text\n\nNQ = [normalize(x[\"question\"]) for x in QA]\n\n# ----------------------------\n# Confidence labeling\n# ----------------------------\ndef confidence_label(score: float, method: str) -> str:\n    \"\"\"\n    Heuristic confidence:\n    - exact: always High\n    - contains: High if >=0.90 else Medium\n    - semantic: High >=0.65, Medium >=0.52, else Low\n    - fuzzy: High >=0.85, Medium >=0.72, else Low\n    \"\"\"\n    m = method.lower()\n    if m == \"exact\":\n        return \"High\"\n    if m == \"contains\":\n        return \"High\" if score >= 0.90 else \"Medium\"\n    if m == \"semantic\":\n        if score >= 0.65:\n            return \"High\"\n        if score >= 0.52:\n            return \"Medium\"\n        return \"Low\"\n    if m == \"fuzzy\":\n        if score >= 0.85:\n            return \"High\"\n        if score >= 0.72:\n            return \"Medium\"\n        return \"Low\"\n    return \"Low\"\n\n# ----------------------------\n# Lexical search\n# ----------------------------\ndef search_lexical(query: str, top_k: int, allow_fuzzy: bool) -> List[Tuple[float, Dict[str, str], str]]:\n    qn = normalize(query)\n    if not qn:\n        return []\n\n    # exact normalized\n    for i, nq in enumerate(NQ):\n        if nq == qn:\n            return [(1.0, QA[i], \"exact\")]\n\n    # contains heuristic\n    contains = []\n    for i, nq in enumerate(NQ):\n        if qn in nq or nq in qn:\n            contains.append((0.92, QA[i], \"contains\"))\n    if contains:\n        return contains[:top_k]\n\n    if not allow_fuzzy:\n        return []\n\n    # fuzzy\n    scored = []\n    for i, nq in enumerate(NQ):\n        s = SequenceMatcher(None, qn, nq).ratio()\n        if s >= FUZZY_THRESHOLD:\n            scored.append((s, QA[i], \"fuzzy\"))\n    scored.sort(key=lambda x: x[0], reverse=True)\n    return scored[:top_k]\n\n# ----------------------------\n# Semantic search (encoder embeddings)\n# ----------------------------\n@dataclass\nclass SemanticIndex:\n    tokenizer: Any\n    model: Any\n    device: torch.device\n    embeddings: Optional[torch.Tensor] = None\n    built: bool = False\n\n    @staticmethod\n    def mean_pool(last_hidden: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n        mask = attention_mask.unsqueeze(-1).type_as(last_hidden)\n        summed = (last_hidden * mask).sum(dim=1)\n        denom = mask.sum(dim=1).clamp(min=1e-9)\n        return summed / denom\n\n    @torch.no_grad()\n    def encode_texts(self, texts: List[str], batch_size: int = 32, max_length: int = 192) -> torch.Tensor:\n        all_vecs = []\n        self.model.eval()\n\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i + batch_size]\n            enc = self.tokenizer(\n                batch,\n                padding=True,\n                truncation=True,\n                max_length=max_length,\n                return_tensors=\"pt\",\n            )\n            enc = {k: v.to(self.device) for k, v in enc.items()}\n            out = self.model(**enc)\n            vec = self.mean_pool(out.last_hidden_state, enc[\"attention_mask\"])\n            vec = F.normalize(vec, p=2, dim=1)\n            all_vecs.append(vec.cpu())\n\n        return torch.cat(all_vecs, dim=0)\n\n    def build(self, questions: List[str], batch_size: int = 32, max_length: int = 192) -> None:\n        t0 = time.time()\n        self.embeddings = self.encode_texts(questions, batch_size=batch_size, max_length=max_length)\n        self.built = True\n        print(f\"‚úÖ Semantic index built: {self.embeddings.shape} in {time.time()-t0:.1f}s\")\n\n    @torch.no_grad()\n    def query(self, text: str, top_k: int = 5, max_length: int = 192) -> List[Tuple[float, int]]:\n        if not self.built or self.embeddings is None:\n            return []\n        q_vec = self.encode_texts([text], batch_size=1, max_length=max_length)[0]\n        sims = (self.embeddings @ q_vec.unsqueeze(1)).squeeze(1)\n        vals, idx = torch.topk(sims, k=min(top_k, sims.shape[0]))\n        return [(float(v.item()), int(i.item())) for v, i in zip(vals, idx)]\n\nSEM = SemanticIndex(\n    tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_NAME),\n    model=AutoModel.from_pretrained(EMBED_MODEL_NAME).to(DEVICE),\n    device=DEVICE,\n)\n\n# ----------------------------\n# GPT fallback generator\n# ----------------------------\n@dataclass\nclass Generator:\n    tokenizer: Any\n    model: Any\n    device: torch.device\n\n    @staticmethod\n    def load(device: torch.device) -> \"Generator\":\n        model_dir = FINETUNED_DIR if os.path.isdir(FINETUNED_DIR) else \"gpt2\"\n        tok = GPT2Tokenizer.from_pretrained(model_dir)\n        tok.pad_token = tok.eos_token\n        mdl = GPT2LMHeadModel.from_pretrained(model_dir).to(device)\n        mdl.eval()\n        return Generator(tok, mdl, device)\n\n    @torch.no_grad()\n    def answer(self, question: str, max_new_tokens: int = 140) -> str:\n        prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n        out = self.model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            top_p=0.9,\n            temperature=0.8,\n            repetition_penalty=1.15,\n            pad_token_id=self.tokenizer.eos_token_id,\n            eos_token_id=self.tokenizer.eos_token_id,\n        )\n        text = self.tokenizer.decode(out[0], skip_special_tokens=True)\n        return text.split(\"### Answer:\", 1)[1].strip() if \"### Answer:\" in text else text.strip()\n\nGEN: Optional[Generator] = None\n\ndef safe_load_generator() -> Generator:\n    global GEN\n    if GEN is not None:\n        return GEN\n    try:\n        GEN = Generator.load(DEVICE)\n        _ = GEN.answer(\"Test?\", max_new_tokens=5)\n        print(f\"‚úÖ Generator loaded on {DEVICE} (from {'./fine_tuned_gpt2' if os.path.isdir(FINETUNED_DIR) else 'gpt2'})\")\n        return GEN\n    except Exception as e:\n        print(\"‚ö†Ô∏è Generator failed on\", DEVICE, \"-> fallback CPU:\", repr(e))\n        GEN = Generator.load(torch.device(\"cpu\"))\n        print(\"‚úÖ Generator loaded on CPU\")\n        return GEN\n\n# ----------------------------\n# Combined search\n# ----------------------------\ndef combined_search(query: str, top_k: int, use_semantic: bool, use_fuzzy: bool, semantic_threshold: float) -> List[Tuple[float, Dict[str, str], str]]:\n    hits: List[Tuple[float, Dict[str, str], str]] = []\n    hits.extend(search_lexical(query, top_k=top_k, allow_fuzzy=use_fuzzy))\n\n    if use_semantic:\n        if not SEM.built:\n            SEM.build([x[\"question\"] for x in QA], batch_size=EMBED_BATCH, max_length=EMBED_MAX_LEN)\n\n        sem_hits = SEM.query(query, top_k=max(top_k, 8), max_length=EMBED_MAX_LEN)\n        for score, idx in sem_hits:\n            if score >= semantic_threshold:\n                hits.append((score, QA[idx], \"semantic\"))\n\n    # de-duplicate by question\n    seen = set()\n    deduped = []\n    for s, item, m in sorted(hits, key=lambda x: x[0], reverse=True):\n        key = normalize(item[\"question\"])\n        if key in seen:\n            continue\n        seen.add(key)\n        deduped.append((s, item, m))\n    return deduped[:top_k]\n\n# ----------------------------\n# UI widgets\n# ----------------------------\ntitle = widgets.HTML(\"<h3>Climate Change Q&A ‚Äî Dataset Search + Confidence + GPT Fallback</h3>\")\n\nquestion_box = widgets.Text(\n    value=\"\",\n    placeholder=\"Type your question here...\",\n    description=\"Question:\",\n    layout=widgets.Layout(width=\"78%\"),\n)\n\nsearch_btn = widgets.Button(\n    description=\"Find Answer\",\n    button_style=\"success\",\n)\n\nbuild_index_btn = widgets.Button(\n    description=\"Build Semantic Index Now\",\n    button_style=\"info\",\n)\n\ntopk_slider = widgets.IntSlider(\n    value=TOPK_DEFAULT,\n    min=1, max=10, step=1,\n    description=\"Top-K:\",\n    continuous_update=False,\n)\n\nmin_score_slider = widgets.FloatSlider(\n    value=SEMANTIC_THRESHOLD_DEFAULT,\n    min=0.20, max=0.80, step=0.01,\n    description=\"Min score:\",\n    continuous_update=False,\n)\n\nuse_semantic_cb = widgets.Checkbox(value=True, description=\"Semantic search\")\nuse_fuzzy_cb = widgets.Checkbox(value=True, description=\"Fuzzy search\")\ngen_fallback_cb = widgets.Checkbox(value=True, description=\"GPT fallback if no match\")\n\n# NEW: Output mode (Dataset / GPT / Both)\nmode_dd = widgets.Dropdown(\n    options=[\"Dataset only\", \"GPT only\", \"Both (compare)\"],\n    value=\"Both (compare)\",\n    description=\"Show:\",\n)\n\noutput = widgets.Output()\n\ndef render_dataset_result(score: float, item: Dict[str, str], method: str) -> None:\n    conf = confidence_label(score, method)\n    display(Markdown(f\"### üìö Dataset Answer ({method}, score `{score:.3f}`, confidence **{conf}**)\"))\n    display(Markdown(f\"**Matched Question:** {item['question']}\"))\n    display(Markdown(f\"**Answer:** {item['answer']}\"))\n\ndef render_gpt_result(question: str) -> None:\n    gen = safe_load_generator()\n    ans = gen.answer(question, max_new_tokens=160)\n    display(Markdown(\"### ü§ñ GPT Answer (generated)\"))\n    display(Markdown(f\"**Answer:** {ans}\"))\n\ndef on_build_index(_):\n    with output:\n        clear_output()\n        if SEM.built:\n            display(Markdown(\"‚úÖ Semantic index already built.\"))\n            return\n        display(Markdown(\"‚è≥ Building semantic index...\"))\n        SEM.build([x[\"question\"] for x in QA], batch_size=EMBED_BATCH, max_length=EMBED_MAX_LEN)\n        display(Markdown(\"‚úÖ Done.\"))\n\ndef on_search(_):\n    with output:\n        clear_output()\n\n        q = question_box.value.strip()\n        if not q:\n            display(Markdown(\"‚ö†Ô∏è Please type a question.\"))\n            return\n\n        semantic_threshold = float(min_score_slider.value)\n\n        hits = combined_search(\n            q,\n            top_k=int(topk_slider.value),\n            use_semantic=bool(use_semantic_cb.value),\n            use_fuzzy=bool(use_fuzzy_cb.value),\n            semantic_threshold=semantic_threshold,\n        )\n\n        show_mode = mode_dd.value\n\n        # If BOTH: show dataset best (if exists) + GPT answer\n        if show_mode == \"Both (compare)\":\n            if hits:\n                best_score, best_item, best_method = hits[0]\n                render_dataset_result(best_score, best_item, best_method)\n            else:\n                display(Markdown(\"### üìö Dataset Answer\"))\n                display(Markdown(\"‚ùå No dataset match found under current thresholds.\"))\n\n            display(Markdown(\"---\"))\n            render_gpt_result(q)\n\n            if hits and len(hits) > 1:\n                display(Markdown(\"---\\n### Other dataset matches\"))\n                for s, item, m in hits[1:]:\n                    conf = confidence_label(s, m)\n                    display(Markdown(f\"- `{m}` score `{s:.3f}` conf **{conf}** ‚Äî **Q:** {item['question']}\"))\n            return\n\n        # Dataset only\n        if show_mode == \"Dataset only\":\n            if hits:\n                best_score, best_item, best_method = hits[0]\n                render_dataset_result(best_score, best_item, best_method)\n\n                if len(hits) > 1:\n                    display(Markdown(\"---\\n### Other matches\"))\n                    for s, item, m in hits[1:]:\n                        conf = confidence_label(s, m)\n                        display(Markdown(f\"- `{m}` score `{s:.3f}` conf **{conf}** ‚Äî **Q:** {item['question']}\"))\n            else:\n                display(Markdown(\"‚ùå No dataset match found under current thresholds.\"))\n                if gen_fallback_cb.value:\n                    display(Markdown(\"---\"))\n                    display(Markdown(\"Fallback enabled, but you chose Dataset only display. Switch 'Show' to GPT/Both.\"))\n            return\n\n        # GPT only\n        if show_mode == \"GPT only\":\n            render_gpt_result(q)\n            if hits:\n                best_score, best_item, best_method = hits[0]\n                conf = confidence_label(best_score, best_method)\n                display(Markdown(\"---\\n### (FYI) Best dataset match\"))\n                display(Markdown(f\"`{best_method}` score `{best_score:.3f}` conf **{conf}** ‚Äî **Q:** {best_item['question']}\"))\n            return\n\nsearch_btn.on_click(on_search)\nbuild_index_btn.on_click(on_build_index)\n\nui = widgets.VBox([\n    title,\n    widgets.HBox([question_box, search_btn]),\n    widgets.HBox([mode_dd, topk_slider, min_score_slider]),\n    widgets.HBox([use_semantic_cb, use_fuzzy_cb, gen_fallback_cb]),\n    build_index_btn,\n    output,\n])\n\ndisplay(ui)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-26T20:22:22.468480Z","iopub.execute_input":"2026-01-26T20:22:22.469143Z","iopub.status.idle":"2026-01-26T20:22:28.076242Z","shell.execute_reply.started":"2026-01-26T20:22:22.469115Z","shell.execute_reply":"2026-01-26T20:22:28.075516Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e68c23c2973c44419f57f4ebf0eea60c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<h3>Climate Change Q&A ‚Äî Dataset Search + Confidence + GPT Fallback</h3>'), HBox(ch‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c04b94ba1fdc4472b74c6ee9e3e05d87"}},"metadata":{}}],"execution_count":8}]}